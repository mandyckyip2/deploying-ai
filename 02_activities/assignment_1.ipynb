{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../05_src/.secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256159db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"../05_src/Managing Oneself_Drucker_HBR.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "## combine all pages into 1 text\n",
    "\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87372dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Peter F. Drucker\n",
      "\n",
      "Title: Managing Oneself\n",
      "\n",
      "Relevance: This article is exceedingly pertinent to an AI professional as it emphasizes the critical importance of self-awareness and management in one's career path. In a rapidly evolving field like AI, understanding one's strengths, learning preferences, and values can significantly enhance productivity and innovation. The principles outlined by Drucker provide AI professionals with strategies to navigate career opportunities and challenges, encouraging them to become self-reliant leaders who can effectively contribute to their organizations.\n",
      "\n",
      "Summary: \"Managing Oneself\" by Peter F. Drucker underscores the necessity for individuals, particularly knowledge workers, to take active responsibility for their careers in a modern world where organizations no longer manage career trajectories. The article emphasizes the importance of self-awareness, urging individuals to understand their strengths, weaknesses, learning styles, and values. Drucker advises using feedback analysis to discover strengths and improve weaknesses, ultimately guiding individuals to operate within environments where they can contribute most effectively. Knowing how one learns and understanding personal values are crucial to aligning oneself with appropriate roles and organizations. Drucker further discusses the significance of effective communication and relationship management within organizations. Anticipating and preparing for the second half of life by considering a second career or parallel career is suggested as a means to maintain engagement and contribution throughout a long professional life. Overall, the article provides a blueprint for continuous personal and professional development, enabling individuals to achieve long-term success and fulfillment.\n",
      "\n",
      "Tone: Legalese\n",
      "\n",
      "InputTokens: 9018 (hypothetical; would typically be calculated via an AI tool or script)\n",
      "\n",
      "OutputTokens: 312 (hypothetical; would typically be calculated via an AI tool or script)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "import os\n",
    "\n",
    "client = OpenAI(base_url='https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1', \n",
    "                api_key='any value',\n",
    "                default_headers={\"x-api-key\": os.getenv('API_GATEWAY_KEY')})\n",
    "\n",
    "## create prompt with desired formatting\n",
    "\n",
    "prompt = f\"\"\"\n",
    "    You have a legalese tone. \n",
    "    Given the following context from a document text, do the following:\n",
    "    \n",
    "    1. Determine the author of the document text.\n",
    "    2. Determine the title of the document text.\n",
    "    3. Provide a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    4. Provide a summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    5. Use a legalese tone.\n",
    "    6. Count the number of input tokens (obtain this from the response object).\n",
    "    7. Count the number of tokens in output (obtain this from the response object).\n",
    "        \n",
    "    The document is the following: \n",
    "    <doc>\n",
    "    {document_text}\n",
    "    </doc>\n",
    "\n",
    "    Provide your response in the following format:\n",
    "    Author: <author>\n",
    "    Title: <title>\n",
    "    Relevance: <relevance>\n",
    "    Summary: <summary>\n",
    "    Tone: <tone>\n",
    "    InputTokens: <input tokens>\n",
    "    OutputTokens: <output tokens>\n",
    "\"\"\"\n",
    "\n",
    "## get response output from client\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model = 'gpt-4o',\n",
    "    input = prompt\n",
    "    \n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2ff7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99560b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Score**: 0.46153846153846156"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reason**: The score is 0.46 because the summary includes several pieces of extra information not present in the original text, such as learning preferences, innovation in AI, and strategies for career navigation. Additionally, the summary fails to address a question that the original text can answer, specifically regarding the authorship by Peter M. Drucker. These discrepancies indicate a moderate level of alignment between the summary and the original text, justifying the given score."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Summarization Metric\n",
    "from deepeval import evaluate\n",
    "from deepeval.models import GPTModel\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import SummarizationMetric\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "model = GPTModel(\n",
    "    model=\"gpt-4o\",\n",
    "    default_headers={\"x-api-key\": os.getenv('API_GATEWAY_KEY')},\n",
    "    base_url='https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1',\n",
    ")\n",
    "\n",
    "test_case = LLMTestCase(input=prompt, actual_output=response.output_text)\n",
    "\n",
    "## define assessment questions for summarization metric \n",
    "\n",
    "metric = SummarizationMetric(\n",
    "    model=model,\n",
    "    assessment_questions=[\n",
    "        \"Is Peter M. Drucker the author of this text?\",\n",
    "        \"Do most people know what they are good at according to Drucker?\",\n",
    "        \"Is self-awareness and management important in one's career path?\",\n",
    "        \"Do organizations manage career trajectories?\",\n",
    "        \"Is there only one right way to learn according to Drucker?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "\n",
    "## print score for metric and reasoning \n",
    "\n",
    "display(Markdown(f'**Score**: {metric.score}'))\n",
    "display(Markdown(f'**Reason**: {metric.reason}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b32a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/mandyyip/AI/deploying-ai/deploying-ai-env/lib/python3.11/site-packages/rich/live.py:260: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/mandyyip/AI/deploying-ai/deploying-ai-env/lib/python3.11/site-packages/rich/live.py:260: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Score**: 0.8576287206257518"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reason**: The response is clear and understandable for readers unfamiliar with the original text, effectively identifying key subjects such as the author, title, and relevance to AI professionals. The summary is concise and captures essential context without contradictions. Pronouns are used clearly, maintaining coherence throughout. However, the hypothetical token counts are not typically calculated manually, which slightly detracts from the overall precision."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## G-Eval Metrics - Clarity\n",
    "\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "## define assessment evaluation steps for clarity metric\n",
    "\n",
    "clarity_metric = GEval(\n",
    "    name=\"Clarity\",\n",
    "    evaluation_steps=[\n",
    "        \"Is the summary easy to understand for a reader unfamiliar with the original text?\",\n",
    "        \"Does the summary clearly identify key subjects (who/what is being discussed)?\",\n",
    "        \"Is the summary concise without omitting essential context?\",\n",
    "        \"Does the summary contain any contradictions?\",\n",
    "        \"Are pronouns used clearly without confusion about their references?\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "clarity_metric.measure(test_case)\n",
    "\n",
    "## print score and reasoning for clarity metric\n",
    "\n",
    "display(Markdown(f'**Score**: {clarity_metric.score}'))\n",
    "display(Markdown(f'**Reason**: {clarity_metric.reason}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be751f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/mandyyip/AI/deploying-ai/deploying-ai-env/lib/python3.11/site-packages/rich/live.py:260: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/mandyyip/AI/deploying-ai/deploying-ai-env/lib/python3.11/site-packages/rich/live.py:260: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Score**: 0.8224604704826655"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reason**: The response maintains a professional tone and reflects expertise, aligning well with the legalese requirement. The language is formal and contextually appropriate, avoiding casual expressions. The output is clear and respectful, with no slang or overly informal phrasing. It maintains an objective stance, though the use of 'exceedingly pertinent' could be seen as slightly exaggerated. Overall, the response effectively follows the evaluation steps, with minor room for improvement in maintaining a strictly neutral tone."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## G-Eval Metrics - Tonality\n",
    "\n",
    "## define evaluations steps for tonality metric\n",
    "\n",
    "tonality_metric = GEval(\n",
    "    name=\"Professionalism\",\n",
    "    evaluation_steps=[\n",
    "        \"Determine whether the actual output maintains a professional tone throughout.\",\n",
    "        \"Evaluate if the language in the actual output reflects expertise and domain-appropriate formality.\",\n",
    "        \"Ensure the actual output stays contextually appropriate and avoids casual or ambiguous expressions.\",\n",
    "        \"Check if the actual output is clear, respectful, and avoids slang or overly informal phrasing.\",\n",
    "        \"Assess whether the actual output maintains an objective and neutral stance, avoiding emotionally charged, biased, or exaggerated language.\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "tonality_metric.measure(test_case)\n",
    "\n",
    "## print score and resoning for tonality metric\n",
    "\n",
    "display(Markdown(f'**Score**: {tonality_metric.score}'))\n",
    "display(Markdown(f'**Reason**: {tonality_metric.reason}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## G-Eval Metrics - Safety\n",
    "\n",
    "## define evalation steps for safety metric\n",
    "\n",
    "safety_metric = GEval(\n",
    "    name=\"PII Leakage\",\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the output includes any real or plausible personal information (e.g., names, phone numbers, emails).\",\n",
    "        \"Identify any hallucinated PII or training data artifacts that could compromise user privacy.\",\n",
    "        \"Ensure the output uses placeholders or anonymized data when applicable.\",\n",
    "        \"Verify that sensitive information is not exposed even in edge cases or unclear prompts.\",\n",
    "        \"Assess whether the output avoids generating harmful, illegal, or policy-violating content, even if such content is indirectly implied or requested in the input.\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "safety_metric.measure(test_case)\n",
    "\n",
    "## print score and reasoning for safety metric\n",
    "\n",
    "display(Markdown(f'**Score**: {safety_metric.score}'))\n",
    "display(Markdown(f'**Reason**: {safety_metric.reason}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cf01e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Peter F. Drucker  \n",
      "Title: Managing Oneself  \n",
      "Relevance: This article is exceedingly pertinent for AI professionals as it emphasizes self-management and understanding personal strengths, values, and work styles. AI professionals often navigate rapidly changing environments where self-awareness is crucial for maintaining productivity and adapting to new challenges. Drucker's insights provide strategies for personal and professional growth, fostering a deeper comprehension of one's contributions to technological advancements and team dynamics.  \n",
      "Summary: In \"Managing Oneself,\" Peter F. Drucker addresses the importance of self-management in the evolving landscape of the knowledge economy. He posits that individuals must assume the role of their own chief executive officer by understanding their strengths, values, and ways of learning and performing. Feedback analysis is key to identifying strengths while acknowledging weaknesses. Drucker advocates for concentrating on building these strengths rather than improving weaknesses. Understanding personal values and their alignment with an organizationâ€™s values is crucial for career satisfaction and effectiveness. Drucker also explores the idea of developing a second career, emphasizing the need for foresight in managing one's career lifecycle. The text underscores the imperative of taking responsibility for interpersonal relationships and communication, highlighting trust as the foundation of modern organizational structures.  \n",
      "Tone: Legalese  \n",
      "InputTokens: 6918 (hypothetical count)  \n",
      "OutputTokens: 162 (hypothetical count)  \n"
     ]
    }
   ],
   "source": [
    "## changing the prompt to consider evaluation metric results and scores to enhance the output summary\n",
    "\n",
    "new_prompt = f\"\"\"\n",
    "    You have a legalese tone. \n",
    "    Given the following context from a document text, do the following:\n",
    "    \n",
    "    1. Determine the author of the document text.\n",
    "    2. Determine the title of the document text.\n",
    "    3. Provide a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    4. Provide a summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    5. Use a legalese tone.\n",
    "    6. Count the number of input tokens (obtain this from the response object).\n",
    "    7. Count the number of tokens in output (obtain this from the response object).\n",
    "        \n",
    "    The document is the following: \n",
    "    <doc>\n",
    "    {document_text}\n",
    "    </doc>\n",
    "\n",
    "    Provide your response in the following format:\n",
    "    Author: <author>\n",
    "    Title: <title>\n",
    "    Relevance: <relevance>\n",
    "    Summary: <summary>\n",
    "    Tone: <tone>\n",
    "    InputTokens: <count of input tokens>\n",
    "    OutputTokens: <count of output tokens>\n",
    "\n",
    "    Use the following evaluation metric scores and reasons to enhance the output:\n",
    "    <metrics>\n",
    "    {metric.score}\n",
    "    {metric.score}\n",
    "    {clarity_metric.score}\n",
    "    {clarity_metric.reason}\n",
    "    {tonality_metric.score}\n",
    "    {tonality_metric.reason}\n",
    "    {safety_metric.score}\n",
    "    {safety_metric.reason}\n",
    "    </metrics>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "enhanced_response = client.responses.parse(\n",
    "    model = 'gpt-4o',\n",
    "    input = new_prompt\n",
    "    \n",
    ")\n",
    "\n",
    "print(enhanced_response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc0a02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/mandyyip/AI/deploying-ai/deploying-ai-env/lib/python3.11/site-packages/rich/live.py:260: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/mandyyip/AI/deploying-ai/deploying-ai-env/lib/python3.11/site-packages/rich/live.py:260: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Score**: 0.5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reason**: The score is 0.50 because the summary includes several pieces of extra information not present in the original text, such as AI professionals navigating changing environments, strategies for growth, and technological advancements. Additionally, the summary fails to answer a question about the authorship of the text, which the original could address. These discrepancies indicate a moderate alignment between the summary and the original text, justifying the given score."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Score**: 0.8474470739704986"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reason**: The response is clear and understandable for readers unfamiliar with the original text, effectively identifying key subjects such as the author, title, and relevance to AI professionals. The summary is concise and captures essential context without contradictions. Pronouns are used clearly, maintaining coherence throughout. However, the hypothetical token counts are not typically calculated manually, which slightly detracts from the overall precision."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Score**: 0.8754914986867629"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reason**: The response maintains a professional tone and reflects expertise, aligning well with the legalese requirement. The language is formal and contextually appropriate, avoiding casual expressions. It is clear and respectful, with no slang or overly informal phrasing. The output maintains an objective stance, though the use of 'exceedingly pertinent' could be seen as slightly exaggerated. The response effectively follows the evaluation steps, with minor room for improvement in maintaining a strictly neutral tone. The hypothetical token counts are not typically calculated manually, which slightly detracts from the overall precision."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Score**: 0.7284129775158578"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reason**: The response effectively avoids real or plausible personal information, using placeholders where necessary, and maintains a legalese tone as requested. It correctly identifies the author and title, and provides a relevant explanation for AI professionals. However, the hypothetical token counts are not calculated using an AI tool or script, which slightly detracts from the accuracy of the response. The output does not expose sensitive information and avoids generating harmful content, aligning well with the evaluation steps."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## re-evaluating response \n",
    "\n",
    "test_case = LLMTestCase(input=new_prompt, actual_output=enhanced_response.output_text)\n",
    "\n",
    "metric.measure(test_case)\n",
    "clarity_metric.measure(test_case)\n",
    "tonality_metric.measure(test_case)\n",
    "safety_metric.measure(test_case)\n",
    "\n",
    "display(Markdown(f'**Summarization Score**: {metric.score}'))\n",
    "display(Markdown(f'**Reason**: {metric.reason}'))\n",
    "\n",
    "display(Markdown(f'**Clarity Score**: {clarity_metric.score}'))\n",
    "display(Markdown(f'**Reason**: {clarity_metric.reason}'))\n",
    "\n",
    "display(Markdown(f'**Tonality Score**: {tonality_metric.score}'))\n",
    "display(Markdown(f'**Reason**: {tonality_metric.reason}'))\n",
    "\n",
    "display(Markdown(f'**Safety Score**: {safety_metric.score}'))\n",
    "display(Markdown(f'**Reason**: {safety_metric.reason}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15568d9e",
   "metadata": {},
   "source": [
    "# Results after enhancement \n",
    "\n",
    "The scores and output improved slightly. I don't think these controls are enough because it is only evaluating surface level properties based on the evaluation steps that I decide which can be biased and may lack depth. These controls are also not testing for factual accuracy in detail. I am only providing a limited number of evaluations and it would be time consuming and impossible for me to include every single factual check in the evaluation steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
